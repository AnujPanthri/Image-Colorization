<<<<<<< HEAD
=======
---
title: Image Colorization
emoji: ðŸ¢
colorFrom: purple
colorTo: yellow
sdk: docker
pinned: false
license: apache-2.0
app_port: 5000
---

hugging face config

>>>>>>> 9806df710f7850bef4ac174b66af98a07744ef4b
## Image Colorization
==============================

An deep learning based Image Colorization project.

## FINDINGS
- the task we want to learn is `image-colorization` but we can accompolish that by doing different types of tasks, I call these **sub-task**, in our content they could be like `regression based image colorization`, `classification(by binning) based colorization`, `GAN based colorization`, `image colorization + scene classication(Let there be colors research paper did this)`.
- based on analysis and while I was trying to come up with a project file structure I came to know that the data, model, loss, metrics, dataloader all these are very coupled while dealing with a particular task(`image-colorization`) but when we talk about a **sub-task** we have much more freedom.
- within a sub-task(e.g., regression-unet-learner) we already made a set of rules and now we can use different models without changing the data, or we can change different datasets while using the same model, **so it is important to fix the sub-task we want to do first.**
- so making a folder for each sub-task seems right as a sub-task has high cohesion and no coupling with any other sub-task.

## RULES
- use **lower_snake_case** for **functions**
- use **lower_snake_case** for **file_name & folder names**
- use **UpperCamelCase** for **class names**
- **sub-task** name should be in **lower-kebab-case**

## Project File Structure
------------
    .
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ external       <- Data from third party sources.
    â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚   â””â”€â”€ raw            <- The original, immutable data dump.
    â”œâ”€â”€ models/             <- Trained models
    â”œâ”€â”€ notebooks/          <- Jupyter notebooks
    â”œâ”€â”€ configs/
    â”‚   â”œâ”€â”€ experiment1.yaml
    â”‚   â”œâ”€â”€ experiment2.yaml
    â”‚   â”œâ”€â”€ experiment3.yaml
    â”‚   â””â”€â”€ ...
    â””â”€â”€ src/
        â”œâ”€â”€ sub_task_1/
        â”‚   â”œâ”€â”€ validate_config.py
        â”‚   â”œâ”€â”€ data/
        â”‚   â”‚   â”œâ”€â”€ register_datasets.py
        â”‚   â”‚   â”œâ”€â”€ datasets/
        â”‚   â”‚   â”‚   â”œâ”€â”€ dataset1.py
        â”‚   â”‚   â”‚   â””â”€â”€ dataset2.py
        â”‚   â”œâ”€â”€ model/
        â”‚   â”‚   â”œâ”€â”€ base_model_interface.py
        â”‚   â”‚   â”œâ”€â”€ register_models.py
        â”‚   â”‚   â”œâ”€â”€ models/
        â”‚   â”‚   â”‚   â”œâ”€â”€ simple_model.py
        â”‚   â”‚   â”‚   â””â”€â”€ complex_model.py
        â”‚   â”‚   â”œâ”€â”€ losses.py
        â”‚   â”‚   â”œâ”€â”€ metrics.py
        â”‚   â”‚   â”œâ”€â”€ callbacks.py
        â”‚   â”‚   â””â”€â”€ dataloader.py
        â”‚   â””â”€â”€ scripts/
        â”‚       â”œâ”€â”€ create_dataset.py
        â”‚       â””â”€â”€ create_model.py
        â”œâ”€â”€ sub_task_2/
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ sub_task_3/
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ scripts/
        â”‚   â”œâ”€â”€ create_sub_task.py
        â”‚   â”œâ”€â”€ prepare_dataset.py
        â”‚   â”œâ”€â”€ visualize_dataset.py
        â”‚   â”œâ”€â”€ visualize_results.py
        â”‚   â”œâ”€â”€ train.py
        â”‚   â”œâ”€â”€ evaluate.py
        â”‚   â””â”€â”€ inference.py
        â””â”€â”€ utils/
            â”œâ”€â”€ data_utils.py
            â””â”€â”€ model_utils.py
--------


<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>


Kaggle API docs:- https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md

## Kaggle Commands:-
- kaggle kernels pull anujpanthri/training-image-colorization-model -p kaggle/
- kaggle kernels push -p kaggle/
- echo "{\"username\":\"$KAGGLE_USERNAME\",\"key\":\"$KAGGLE_KEY\"}" > kaggle.json

## Docker Commands:-
- docker buildx build --secret id=COMET_API_KEY,env=COMET_API_KEY -t testcontainer
- docker run -it -p 5000:5000 -e COMET_API_KEY=$COMET_API_KEY testcontainer

## Git Commands:-
- git lfs migrate info --everything --include="*.zip,*.png,*.jpg"
- git lfs migrate import --everything --include="*.zip,*.png,*.jpg"

### Version 1:

- im gonna skip logging for now and rather use print statements


## Dataset

![](outputs/artifacts/dataset/trainval_image.png)
![](outputs/artifacts/dataset/test_image.png)

## Result

![](outputs/artifacts/result/train_image.png)
![](outputs/artifacts/result/val_image.png)
