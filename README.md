---
title: Image Colorization
emoji: üê¢
colorFrom: purple
colorTo: yellow
sdk: docker
pinned: false
license: apache-2.0
app_port: 5000
---

hugging face config

## Image Colorization
==============================

An deep learning based Image Colorization project.

## FINDINGS
- the task we want to learn is `image-colorization` but we can accompolish that by doing different types of tasks, I call these **sub-task**, in our content they could be like `regression based image colorization`, `classification(by binning) based colorization`, `GAN based colorization`, `image colorization + scene classication(Let there be colors research paper did this)`.
- based on analysis and while I was trying to come up with a project file structure I came to know that the data, model, loss, metrics, dataloader all these are very coupled while dealing with a particular task(`image-colorization`) but when we talk about a **sub-task** we have much more freedom.
- within a sub-task(e.g., regression-unet-learner) we already made a set of rules and now we can use different models without changing the data, or we can change different datasets while using the same model, **so it is important to fix the sub-task we want to do first.**
- so making a folder for each sub-task seems right as a sub-task has high cohesion and no coupling with any other sub-task.

## RULES
- use **lower_snake_case** for **functions**
- use **lower_snake_case** for **file_name & folder names**
- use **UpperCamelCase** for **class names**
- **sub-task** name should be in **lower-kebab-case**

## Project File Structure
------------
    .
    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data/
    ‚îÇ   ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ   ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ   ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ   ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îú‚îÄ‚îÄ models/             <- Trained models
    ‚îú‚îÄ‚îÄ notebooks/          <- Jupyter notebooks
    ‚îú‚îÄ‚îÄ configs/
    ‚îÇ   ‚îú‚îÄ‚îÄ experiment1.yaml
    ‚îÇ   ‚îú‚îÄ‚îÄ experiment2.yaml
    ‚îÇ   ‚îú‚îÄ‚îÄ experiment3.yaml
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ sub_task_1/
        ‚îÇ   ‚îú‚îÄ‚îÄ validate_config.py
        ‚îÇ   ‚îú‚îÄ‚îÄ data/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register_datasets.py
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets/
        ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset1.py
        ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset2.py
        ‚îÇ   ‚îú‚îÄ‚îÄ model/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_model_interface.py
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ register_models.py
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
        ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_model.py
        ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ complex_model.py
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ losses.py
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ callbacks.py
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataloader.py
        ‚îÇ   ‚îî‚îÄ‚îÄ scripts/
        ‚îÇ       ‚îú‚îÄ‚îÄ create_dataset.py
        ‚îÇ       ‚îî‚îÄ‚îÄ create_model.py
        ‚îú‚îÄ‚îÄ sub_task_2/
        ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îú‚îÄ‚îÄ sub_task_3/
        ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îú‚îÄ‚îÄ scripts/
        ‚îÇ   ‚îú‚îÄ‚îÄ create_sub_task.py
        ‚îÇ   ‚îú‚îÄ‚îÄ prepare_dataset.py
        ‚îÇ   ‚îú‚îÄ‚îÄ visualize_dataset.py
        ‚îÇ   ‚îú‚îÄ‚îÄ visualize_results.py
        ‚îÇ   ‚îú‚îÄ‚îÄ train.py
        ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py
        ‚îÇ   ‚îî‚îÄ‚îÄ inference.py
        ‚îî‚îÄ‚îÄ utils/
            ‚îú‚îÄ‚îÄ data_utils.py
            ‚îî‚îÄ‚îÄ model_utils.py
--------


<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>


Kaggle API docs:- https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md

## Kaggle Commands:-
- kaggle kernels pull anujpanthri/training-image-colorization-model -p kaggle/
- kaggle kernels push -p kaggle/
- echo "{\"username\":\"$KAGGLE_USERNAME\",\"key\":\"$KAGGLE_KEY\"}" > kaggle.json

## Docker Commands:-
- docker buildx build --secret id=COMET_API_KEY,env=COMET_API_KEY -t testcontainer
- docker run -it -p 5000:5000 -e COMET_API_KEY=$COMET_API_KEY testcontainer

## Git Commands:-
- git lfs migrate info --everything --include="*.zip,*.png,*.jpg"
- git lfs migrate import --everything --include="*.zip,*.png,*.jpg"

### Version 1:

- im gonna skip logging for now and rather use print statements


## Dataset

![](outputs/artifacts/dataset/trainval_image.png)
![](outputs/artifacts/dataset/test_image.png)

## Result

![](outputs/artifacts/result/train_image.png)
![](outputs/artifacts/result/val_image.png)
